| **Requirement**                        | **Description**                                                                                       | **Example**                                                                                                  | **Completed (Y/N)** |
|----------------------------------------|-------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------|--------------------|
| **1. Identify AI System Risk Level**    | Classify your AI system as prohibited, high-risk, limited-risk, or minimal-risk according to the AI Act.| Assess if your AI system falls under high-risk areas like healthcare, law enforcement, or employment.         |                    |
| **2. Risk Management System**          | Implement a risk management system to identify, assess, and mitigate risks related to the AI system.     | Regularly evaluate your AI system for risks such as bias, data inaccuracies, or security vulnerabilities.     |                    |
| **3. High-Quality Data Governance**    | Ensure that training, validation, and testing datasets are of high quality, relevant, and free from bias.| Use diverse datasets to avoid bias in hiring or credit scoring AI systems.                                   |                    |
| **4. Technical Documentation**         | Maintain detailed documentation covering the design, development, testing, and operation of the AI system.| Create a technical document that outlines the AI system's architecture, data sources, and testing protocols.   |                    |
| **5. Record Keeping and Logging**      | Keep logs and records of all AI system operations to ensure traceability and accountability.            | Log all AI decisions made in areas like financial services for future auditing.                               |                    |
| **6. Transparency and User Information**| Ensure users are informed about the system's purpose, limitations, and risks.                            | Provide clear user guides and system limitations for a healthcare diagnosis AI tool.                          |                    |
| **7. Human Oversight**                 | Implement mechanisms for human oversight to monitor and intervene in AI system operations if necessary. | Enable human operators to override AI decisions in autonomous vehicle systems if safety is compromised.        |                    |
| **8. Robustness, Accuracy, and Security**| Ensure the AI system is robust, accurate, secure, and tested for resilience against attacks.             | Perform regular security tests to protect a facial recognition AI system from adversarial attacks.            |                    |
| **9. Conformity Assessment**           | Conduct a conformity assessment to verify that the AI system meets the AI Act's regulatory requirements.  | Perform internal reviews or hire third-party auditors to certify an AI system used in biometric identification. |                    |
| **10. Post-Market Monitoring**         | Monitor the AI system after deployment to identify and mitigate any emerging risks or issues.            | Set up a process to track the performance of an AI-powered medical device in real-world conditions.            |                    |
| **11. Reporting Serious Incidents**    | Establish a process for reporting serious incidents or malfunctions to relevant authorities.              | Report any safety-related malfunctions in AI systems used in autonomous drones to the appropriate regulator.    |                    |
| **12. Compliance of AI Components**    | Ensure that all AI system components, including third-party services and data, comply with the AI Act.   | Verify that external data used for training your AI model is obtained and processed in compliance with regulations.|                    |

